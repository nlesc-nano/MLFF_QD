# ==============================================================================
# Notes for users:
#
# - Use **dot notation** for override keys (see below for example).
# - In case of conflict, keys from `common` always take priority over `overrides`.
# - Keys not present in the engine template will be ignored (with a warning).
# - n_rbf: For schnet, painn, fusion → RBF basis functions. For nequip/allegro → mapped to bessel basis.
# ==============================================================================

platform: fusion  # [schnet, painn, fusion, nequip, allegro, mace]

common:
  data:
    input_xyz_file: ./basic_consolidated_dataset_1000CdSe.xyz   # Path to your XYZ file

  model:
    mp_layers: 3       # Number of message-passing layers in the neural network.
    features: 32       # Dimension of atomic feature vectors (per-atom embedding size).
    cutoff: 12.0
    n_rbf: 8           # For schnet, painn, fusion: number of RBF (radial basis functions)
                       # For nequip, allegro: this value will be mapped to number of bessel basis functions
    l_max: 1
    parity: true
    model_dtype: float32
    chemical_symbols: [Cd, Se, Cl]
    # pair_potential: Option to enable ZBL for NequIP/Allegro models only.
    #   - Set to "ZBL" (as a string) to ENABLE ZBL pair potential
    #   - Set to null to DISABLE the pair potential block (recommended for most cases)
    #   - Any other value will raise an error
    pair_potential: null   # Use "ZBL" (string), or null to disable

  training:
    seed: 42
    train_size: 0.8
    val_size: 0.1
    test_size: 0.1
    batch_size: 16      # Global batch size for training. 
                        # If using multiple GPUs, this value is split evenly across devices 
                        # (e.g., 16 total → 8 per GPU when devices=2). 
                        # For a single GPU, the full batch size is used.
    epochs: 3
    learning_rate: 0.001
    num_workers: 24
    accelerator: cuda
    devices: 2              # Number of GPUs to use for distributed or data-parallel training. Set 1 for single-GPU.
    log_every_n_steps: 5    # Frequency (in steps) for logging metrics and losses to the console or logger.
    optimizer: AdamW
    scheduler:
      type: ReduceLROnPlateau
      factor: 0.8
      patience: 5
    pin_memory: true      # If true, preloads data into page-locked memory for faster GPU transfer.
    
    early_stopping:
      enabled: true       # Enable or disable early stopping to avoid overfitting.
      patience: 30        # Stop training if validation loss doesn’t improve for this many epochs.
      min_delta: 0.003    # Minimum change in monitored metric to qualify as improvement.
      monitor: val_loss
      # monitor: val_loss         # for schnet
      # monitor: val0_epoch/weighted_sum   # for nequip
      # (If omitted, the code auto-inserts the correct default!)

  loss:
    energy_weight: 0.05
    forces_weight: 0.95

  output:
    output_dir: ./resultsNewNewX
   
  fine_tuning:
    enabled: false                  # If false, standard training runs
    pretrained_model: "" # Model Path
    learning_rate: 1.0e-4         
    early_stopping_patience: 10    
    freeze_backbone: true          # Unified flag: Freezes representation layers (Only for Schnet and Painn)
    
    # Optional advanced args (NequIP specific), we should not put it here 
    # per_species_shifts: 
      # Cd: -150.0
      # Se: -13.0

# ------------------------------------------------------------------------------
# Overrides section:
# - Use dot notation for all keys (e.g., model.n_rbf, trainer.callbacks[0].patience)
# - Only specify keys you want to override for a specific engine.
# - If a key is in both `common` and `overrides`, `common` wins.
# ------------------------------------------------------------------------------

overrides:

  schnet:
    model.n_rbf: 30
    model.activation: relu
    model.n_layers: 1
    model.dropout: 0.2
    logging.folder: ./resultsExpert
    trainer:
      callbacks:
        - _target_: lightning.pytorch.callbacks.EarlyStopping
          patience: 100

  nequip:
    model.num_bessels: 50                     # dot notation for nested keys
    training_module.model.parity: false        # disables parity in the model
    model.n_layers: 5                         # ignored if mp_layers is set in common
    training_module.model.num_layers: 5        # ignored if mp_layers is set in common
    model.activation: relu                     # ignored if not in template
    model.dropout: 0.1                         # ignored if not in template
    logging.folder: ./resultsNequIP
    trainer.logger[0].save_dir: logsNequIPX
    trainer.callbacks[1].filename: bestNew
    trainer.callbacks[0].patience: 100

    # Example: Add new parameter not present in template to see warning in logs
    model.new_param: 12345
    training_module.loss.coeffs.total_energy: 0.02

  painn:
    model.n_atom_basis: 50
    training.num_val: 0.3
    outputs.forces.loss_weight: 0.91
    logging.folder: ./resultsPainn
    trainer.callbacks[1].monitor: val_loss
    trainer.logger[0].save_dir: ./logsPainnX
    fine_tuning.lr: 0.05

  fusion:
    model.n_interactions: 4
    training.num_train: 0.65
    outputs.energy.loss_weight: 0.09
    logging.folder: ./resultsFusion
    trainer.callbacks[0].min_delta: 0.01
    trainer.logger[0].save_dir: ./logsFusionX

  allegro:
    training_module.model.radial_chemical_embed.num_bessels: 17
    model.n_bessels: 50
    model.n_rbf: 30
    training_module.model.num_scalar_features: 48
    training_module.model.l_max: 2
    training_module.model.parity: false
    trainer.callbacks[0].patience: 10
    trainer.callbacks[2].logging_interval: epoch
    trainer.logger[0].save_dir: ./logsAllegroX

  mace:
    num_channels: 64
    model.n_rbf: 30
    max_L: 1
    lr: 0.007
    eval_interval: 10
    valid_file: ./converted_data/mace_val.xyz

