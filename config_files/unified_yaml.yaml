platform: fusion  # Options: allegro, mace, nequip, schnet, painn, fusion

common:
  p_mlff_qd_input_xyz: ./basic_consolidated_dataset_1000CdSe.xyz # Basic XYZ file to be converted for all platforms
  dataset_path:   # Path to the dataset file
  results_dir: ./results  # Directory for results and logs
  seed: 42  # Random seed for reproducibility
  n_train: 800  # Number of training data
  n_val: 200  # Number of validation data
  batch_size: 16  # Batch size for training
  max_epochs: 3  # Maximum number of training epochs

allegro:
    run: [train, test]

    cutoff_radius: 12.0
    chemical_symbols: [Cd, Cl, Se] 
    model_type_names: ${chemical_symbols}

    data:
      _target_: nequip.data.datamodule.ASEDataModule
      seed: 456             
      split_dataset:
        file_path: 
        train: 0.8
        val: 0.1
        test: 0.1
      transforms:
        - _target_: nequip.data.transforms.NeighborListTransform
          r_max: ${cutoff_radius}
        - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
          chemical_symbols: ${chemical_symbols}
      
      train_dataloader:
        _target_: torch.utils.data.DataLoader
        batch_size: 16
      val_dataloader:
        _target_: torch.utils.data.DataLoader
        batch_size: 16
      test_dataloader: ${data.val_dataloader}
      stats_manager:
        _target_: nequip.data.CommonDataStatisticsManager
        type_names: ${model_type_names}

    trainer:
      _target_: lightning.Trainer
      max_epochs: 3
      check_val_every_n_epoch: 1
      log_every_n_steps: 5
      callbacks:
        - _target_: lightning.pytorch.callbacks.ModelCheckpoint
          dirpath: ${hydra:runtime.output_dir}
          save_last: true


    # NOTE:
    # interpolation parameters for Allegro model
    num_scalar_features: 64


    training_module:
      _target_: nequip.train.EMALightningModule
      loss:
        _target_: nequip.train.EnergyForceLoss
        per_atom_energy: true
        coeffs:
          total_energy: 0.05
          forces: 0.95
      val_metrics:
        _target_: nequip.train.EnergyForceMetrics
        coeffs:
          per_atom_energy_mae: 0.05
          forces_mae: 0.95
      test_metrics: ${training_module.val_metrics}
      optimizer:
        _target_: torch.optim.Adam
        lr: 0.001
      # ^ IMPORTANT: Allegro models do better with learning rates around 1e-3

      # to use the Allegro model in the NequIP framework, the following `model` block has to be changed to be that of Allegro's
      model:
        _target_: allegro.model.AllegroModel

        # === basic model params ===
        seed: 456
        model_dtype: float32
        type_names: ${model_type_names}
        r_max: ${cutoff_radius}

        # === two-body scalar embedding ===
        radial_chemical_embed:
          # the defaults for the Bessel embedding module are usually appropriate
          _target_: allegro.nn.TwoBodyBesselScalarEmbed
          num_bessels: 8
          bessel_trainable: false
          polynomial_cutoff_p: 6

        # output dimension of the radial-chemical embedding
        radial_chemical_embed_dim: ${num_scalar_features}

        # scalar embedding MLP
        scalar_embed_mlp_hidden_layers_depth: 1
        scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
        scalar_embed_mlp_nonlinearity: silu

        # === core hyperparameters ===
        # The following hyperparameters are the main ones that one should focus on tuning.

        # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
        l_max: 1

        # number of tensor product layers, 1-3 usually best, more is more accurate but slower
        num_layers: 2

        # number of scalar features, more is more accurate but slower
        # 16, 32, 64, 128, 256 are good options to try depending on the dataset
        num_scalar_features: ${num_scalar_features}

        # number of tensor features, more is more accurate but slower
        # 8, 16, 32, 64 are good options to try depending on the dataset
        num_tensor_features: 32

        # == allegro MLPs ==
        # neural network parameters in the Allegro layers
        allegro_mlp_hidden_layers_depth: 1
        allegro_mlp_hidden_layers_width: ${num_scalar_features}
        allegro_mlp_nonlinearity: silu
        # ^ setting `nonlinearity` to `null` means that the Allegro MLPs are effectively linear layers

        # === advanced hyperparameters ===
        # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

        # whether to include features with odd mirror parity
        # often turning parity off gives equally good results but faster networks, so do consider this
        parity: true

        # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
        # default is `true`, which is expected to be more expressive than `false`
        tp_path_channel_coupling: true

        # == readout MLP ==
        # neural network parameters in the readout layer
        readout_mlp_hidden_layers_depth: 1
        readout_mlp_hidden_layers_width: ${num_scalar_features}
        readout_mlp_nonlinearity: silu
        # ^ setting `nonlinearity` to `null` means that output MLP is effectively a linear layer

        # === misc hyperparameters ===
        # average number of neighbors for edge sum normalization
        avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

        # per-type per-atom scales and shifts
        per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
        # ^ this should typically be the isolated atom energies for your dataset
        #   provided as a dict, e.g.
        # per_type_energy_shifts: 
        #   C: 1.234
        #   H: 2.345
        #   O: 3.456
        per_type_energy_scales: ${training_data_stats:forces_rms}
        per_type_energy_scales_trainable: false
        per_type_energy_shifts_trainable: false

        # ZBL pair potential (optional, can be removed or included depending on aplication)
        # see NequIP docs for details:
        # https://nequip.readthedocs.io/en/latest/api/nn.html#nequip.nn.pair_potential.ZBL
        pair_potential:
          _target_: nequip.nn.pair_potential.ZBL
          units: real     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
          chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types


    global_options:
      allow_tf32: false
mace:
  # ===========================
  # Experiment & Paths
  # ===========================
  name: mace_cdsecl_model  # Experiment name
  seed: 42  # Random seed for reproducibility
  log_level: INFO  # Logging level
  error_table: PerAtomMAE  # Report validation metrics using MAE

  # ===========================
  # Hardware & Precision
  # ===========================
  device: cuda  # Options: cpu, cuda, mps, xpu
  default_dtype: float32  # Default data type

  # ===========================
  # Dataset & Keys
  # ===========================
  train_file:   # Training dataset file
  valid_file: null  # Validation dataset file
  test_file: null  # Test dataset file

  energy_key: energy  # Key for energy in dataset
  forces_key: forces  # Key for forces in dataset
  stress_key: null  # Key for stress in dataset

  valid_fraction: 0.2  # Fraction of data for validation
  batch_size: 16  # Batch size for training
  num_workers: 24  # Number of data loader workers
  pin_memory: true  # Enables faster CPU â†’ GPU transfer

  # ===========================
  # Model Configuration
  # ===========================
  model: MACE  # Model type
  r_max: 12  # Cutoff radius
  num_radial_basis: 20  # Number of radial basis functions
  num_cutoff_basis: 6  # Number of cutoff basis functions
  max_ell: 3  # Maximum angular momentum
  num_channels: 64  # Number of atom basis channels
  max_L: 2  # Maximum spherical harmonic degree
  num_interactions: 3  # Number of interaction layers
  correlation: 3  # Correlation order
  avg_num_neighbors: 100.80  # Average number of neighbors

  # ===========================
  # Training Parameters
  # ===========================
  max_num_epochs: 3  # Maximum number of epochs
  ema: true  # Use exponential moving average
  ema_decay: 0.99  # EMA decay rate

  # ===========================
  # Validation & Early Stopping
  # ===========================
  valid_batch_size: 16  # Batch size for validation
  eval_interval: 1  # Check validation every 1 epoch
  patience: 30  # Early stop if no improvement in 30 checks

  # ===========================
  # Stochastic Weight Averaging
  # ===========================
  swa: true  # Enable stochastic weight averaging
  start_swa: 400  # Epoch to start SWA
  swa_energy_weight: 1.0  # SWA weight for energy
  swa_forces_weight: 100.0  # SWA weight for forces

  # ===========================
  # Loss Weights
  # ===========================
  forces_weight: 0.95  # Weight for forces in loss
  energy_weight: 0.05  # Weight for energy in loss

  # ===========================
  # Optimizer & Scheduler
  # ===========================
  optimizer: adam  # Optimizer type
  lr: 0.001  # Learning rate
  weight_decay: 1e-5  # Weight decay

  scheduler: ReduceLROnPlateau  # Scheduler type
  lr_factor: 0.8  # Factor to reduce learning rate
  scheduler_patience: 5  # Patience for scheduler
  lr_scheduler_gamma: 0.9993  # Gamma for learning rate decay

  # ===========================
  # Energy Baseline & Scaling
  # ===========================
  E0s: "average"  # Use average per-atom energy
  scaling: rms_forces_scaling  # Scaling method
  compute_avg_num_neighbors: true  # Compute average number of neighbors

nequip:

    run: [train, val, test]
    cutoff_radius: 12.0
    
    chemical_symbols: [Cd, Cl, Se] 
    model_type_names: ${chemical_symbols}
    
    data:
      _target_: nequip.data.datamodule.ASEDataModule
      seed: 456             # dataset seed for reproducibility
      
      # here we take an ASE-readable file (in extxyz format) and split it into train:val:test = 80:10:10
      split_dataset:
        file_path: 
        train: 0.8
        val: 0.1
        test: 0.1
      
      transforms:
        - _target_: nequip.data.transforms.NeighborListTransform
          r_max: ${cutoff_radius}
        - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
          chemical_symbols: ${chemical_symbols}

      train_dataloader:
        _target_: torch.utils.data.DataLoader
        batch_size: 16
        num_workers: 5
        shuffle: true
      val_dataloader:
        _target_: torch.utils.data.DataLoader
        batch_size: 16
        num_workers: ${data.train_dataloader.num_workers}  # we want to use the same num_workers -- variable interpolation helps
      test_dataloader: ${data.val_dataloader}  # variable interpolation comes in handy again

      stats_manager:

        _target_: nequip.data.CommonDataStatisticsManager

        dataloader_kwargs:
          batch_size: 16

        type_names: ${model_type_names}

    trainer:
      _target_: lightning.Trainer
      accelerator: auto
      enable_checkpointing: true
      max_epochs: 3
      max_time: 03:00:00:00
      check_val_every_n_epoch: 1  # how often to validate
      log_every_n_steps: 1       # how often to log

      # use any Lightning supported logger
      logger:
        - _target_: lightning.pytorch.loggers.TensorBoardLogger
          save_dir: ./results
          name: lightning_logs
          version: null
          default_hp_metric: false

      callbacks:
        # stop training when some criterion is met
        - _target_: lightning.pytorch.callbacks.EarlyStopping
          monitor: val0_epoch/weighted_sum        # validation metric to monitor
          min_delta: 1e-3                         # how much to be considered a "change"
          patience: 20                            # how many instances of "no change" before stopping

        # checkpoint based on some criterion
        - _target_: lightning.pytorch.callbacks.ModelCheckpoint
          monitor: val0_epoch/weighted_sum        # validation metric to monitor
          dirpath: ./results    
          filename: best                          # best.ckpt is the checkpoint name
          save_last: true                         # last.ckpt will be saved
          
        # log learning rate, e.g. to monitor what the learning rate scheduler is doing
        - _target_: lightning.pytorch.callbacks.LearningRateMonitor
          logging_interval: epoch

    # training_module refers to a NequIPLightningModule
    training_module:
      _target_: nequip.train.EMALightningModule
      
      ema_decay: 0.999
      loss:
        _target_: nequip.train.EnergyForceLoss
        per_atom_energy: true
        coeffs:
          total_energy: 0.05
          forces: 0.95

      val_metrics:
        _target_: nequip.train.EnergyForceMetrics
        coeffs:
          total_energy_mae: 1.0
          forces_mae: 1.0
          # keys `total_energy_rmse` and `forces_rmse`, `per_atom_energy_rmse` and `per_atom_energy_mae` are also available

      # we could have train_metrics and test_metrics be different from val_metrics, but it makes sense to have them be the same
      train_metrics: ${training_module.val_metrics}  # use variable interpolation
      test_metrics: ${training_module.val_metrics}  # use variable interpolation

      # any torch compatible optimizer: https://pytorch.org/docs/stable/optim.html#algorithms
      optimizer:
        _target_: torch.optim.Adam
        lr: 0.03

      lr_scheduler:
        # any torch compatible lr sceduler
        scheduler:
          _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
          factor: 0.6
          patience: 5
          threshold: 0.2
          min_lr: 1e-6
        monitor: val0_epoch/weighted_sum
        interval: epoch
        frequency: 1

      # model details
      model:
        _target_: nequip.model.NequIPGNNModel

        # == basic model params ==
        seed: 456
        model_dtype: float32
        type_names: ${model_type_names}
        r_max: ${cutoff_radius}

        # == bessel encoding ==
        num_bessels: 8                # number of basis functions used in the radial Bessel basis, the default of 8 usually works well
        bessel_trainable: false       # set true to train the bessel weights (default false)
        polynomial_cutoff_p: 6        # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

        # == convnet layers ==
        num_layers: 3       # number of interaction blocks, we find 3-5 to work best
        l_max: 1            # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower
        parity: true        # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this
        num_features: 32    # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower

        # == radial network ==
        radial_mlp_depth: 2         # number of radial layers, usually 1-3 works best, smaller is faster
        radial_mlp_width: 64        # number of hidden neurons in radial function, smaller is faster
        
        # average number of neighbors for edge sum normalization
        avg_num_neighbors: ${training_data_stats:num_neighbors_mean}
        
        # == per-type per-atom scales and shifts ==
        per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
        per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
        per_type_energy_scales_trainable: false
        per_type_energy_shifts_trainable: false

        # == ZBL pair potential ==
        pair_potential:
          _target_: nequip.nn.pair_potential.ZBL
          units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
          chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

    # global options
    global_options:
      allow_tf32: false

schnet:
  # General settings
  general:
    seed: 42  # Random seed for reproducibility
    database_name: CdSe.db  # Name of the database file

  # Data handling settings
  data:
    dataset_path:   # Path to dataset
    use_last_n: 100  # Current testing

  # Model architecture settings
  model:
    model_type: schnet  # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.0  # Cutoff radius
    n_rbf: 40  # Number of radial basis functions
    n_atom_basis: 192  # Number of atom basis functions
    n_interactions: 2  # Number of interaction blocks
    dropout_rate: null  # Default is 0.1, null for 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null  # If null, n_neurons == n_atom_basis
    distance_unit: Ang  # Distance unit
    property_unit_dict:
      energy: eV  # Energy unit
      forces: eV/Ang  # Forces unit

  # Output settings
  outputs:
    energy:
      loss_weight: 0.05  # Loss weight for energy
      metrics: MAE  # Metrics for energy
    forces:
      loss_weight: 0.95  # Loss weight for forces
      metrics: MAE  # Metrics for forces

  # Training settings
  training:
    accelerator: gpu  # Accelerator type
    devices: 1  # Number of devices
    precision: 32  # Precision for computations
    batch_size: 16  # Batch size
    num_train: 800  # Number of training samples
    num_val: 200  # Number of validation samples
    max_epochs: 3  # Maximum epochs
    num_workers: 24  # Number of data loader workers
    pin_memory: true  # Enable fast CPU to GPU transfer
    optimizer:
      type: AdamW  # Optimizer type
      lr: 0.0001  # Learning rate (changed from 0.0001 to 0.001 in comment)
    scheduler:
      type: ReduceLROnPlateau  # Scheduler type
      factor: 0.8  # Factor to reduce learning rate
      patience: 30  # Patience for scheduler
      verbose: true  # Verbose output

  # Logging and checkpoint settings
  logging:
    folder: ./results  # Results folder
    log_dir: lightning_logs  # Directory for logs
    checkpoint_dir: best_inference_model  # Directory for checkpoints
    monitor: val_loss  # Metric to monitor

  # Testing settings
  testing:
    trained_model_path: ./results  # Path to load trained model
    csv_file_name: actual_vs_predicted_enrgforc.csv  # Path to save predictions CSV

  # Resume training settings
  resume_training:
    resume_checkpoint_dir: null  # Path to model checkpoint file if required

  # Fine Tuning settings
  fine_tuning:
    pretrained_checkpoint: null  # Checkpoint file path
    freeze_embedding: true  # Freeze embedding layers
    freeze_interactions_up_to: 2  # Number of interaction layers to freeze
    freeze_all_representation: true  # Freeze all representation layers
    lr: 0.00005  # Learning rate for fine-tuning
    early_stopping_patience: 10  # Patience for early stopping
    best_model_dir: fine_tuned_best_model  # Subdirectory for best model
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory for checkpoints
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
